import os
import random
from functools import partial

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

import timm
import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.utils.data import DataLoader

import torchmetrics

from transformers import AutoModel, AutoTokenizer

from dataset import MultimodalDataset, collate_fn, get_transforms
from config import Config as config


class TrainingVisualizer:
    def __init__(self):
        self.train_losses = []
        self.train_mae_scores = []
        self.val_mae_scores = []
        self.train_rmse_scores = []
        self.val_rmse_scores = []
        self.epochs = []
        
    def update(self, epoch, train_loss, train_mae, val_mae, train_rmse=None, val_rmse=None):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏"""
        self.epochs.append(epoch)
        self.train_losses.append(train_loss)
        self.train_mae_scores.append(train_mae)
        self.val_mae_scores.append(val_mae)
        if train_rmse is not None:
            self.train_rmse_scores.append(train_rmse)
        if val_rmse is not None:
            self.val_rmse_scores.append(val_rmse)
    
    def plot_metrics(self, save_path=None):
        """–°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ–∏–∫–∏ loss –∏ –º–µ—Ç—Ä–∏–∫ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # –ì—Ä–∞—Ñ–∏–∫ Loss
        ax1.plot(self.epochs, self.train_losses, 'b-', label='Train Loss', linewidth=2)
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.set_title('Training Loss (MSE)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ MAE
        ax2.plot(self.epochs, self.train_mae_scores, 'g-', label='Train MAE', linewidth=2)
        ax2.plot(self.epochs, self.val_mae_scores, 'r-', label='Val MAE', linewidth=2)
        if self.train_rmse_scores:
            ax2.plot(self.epochs, self.train_rmse_scores, 'g--', label='Train RMSE', alpha=0.7)
            ax2.plot(self.epochs, self.val_rmse_scores, 'r--', label='Val RMSE', alpha=0.7)
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Error (calories)')
        ax2.set_title('Regression Metrics Progress')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"–ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {save_path}")
        
        plt.show()
    
    def print_current_metrics(self, epoch, train_loss, train_mae, val_mae, train_rmse=None, val_rmse=None):
        """–ü–µ—á–∞—Ç–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏ —Å —Ü–≤–µ—Ç–æ–≤—ã–º –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ–º"""
        loss_color = '\033[94m'  # —Å–∏–Ω–∏–π
        train_color = '\033[92m'  # –∑–µ–ª–µ–Ω—ã–π
        val_color = '\033[93m'    # –∂–µ–ª—Ç—ã–π
        reset_color = '\033[0m'   # —Å–±—Ä–æ—Å
        
        metrics_str = (f"Epoch {epoch:2d}/{config.EPOCHS} | "
                       f"{loss_color}Loss: {train_loss:.2f}{reset_color} | "
                       f"{train_color}Train MAE: {train_mae:.1f} kcal{reset_color} | "
                       f"{val_color}Val MAE: {val_mae:.1f} kcal{reset_color}")
        
        if train_rmse is not None and val_rmse is not None:
            metrics_str += f" | {train_color}Train RMSE: {train_rmse:.1f}{reset_color} | {val_color}Val RMSE: {val_rmse:.1f}{reset_color}"
        
        print(metrics_str)
    
    def save_metrics_to_file(self, filename="training_metrics.csv"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –≤ CSV —Ñ–∞–π–ª"""
        metrics_df = pd.DataFrame({
            'epoch': self.epochs,
            'train_loss': self.train_losses,
            'train_mae': self.train_mae_scores,
            'val_mae': self.val_mae_scores,
            'train_rmse': self.train_rmse_scores if self.train_rmse_scores else [None] * len(self.epochs),
            'val_rmse': self.val_rmse_scores if self.val_rmse_scores else [None] * len(self.epochs)
        })
        
        metrics_df.to_csv(filename, index=False)
        print(f"–ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {filename}")


def seed_everything(seed: int):
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.benchmark = True


def set_requires_grad(module: nn.Module, unfreeze_pattern="", verbose=False):
    if len(unfreeze_pattern) == 0:
        for _, param in module.named_parameters():
            param.requires_grad = False
        return

    pattern = unfreeze_pattern.split("|")

    for name, param in module.named_parameters():
        if any([name.startswith(p) for p in pattern]):
            param.requires_grad = True
            if verbose:
                print(f"–†–∞–∑–º–æ—Ä–æ–∂–µ–Ω —Å–ª–æ–π: {name}")
        else:
            param.requires_grad = False


class CrossModalAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.embed_dim = config.HIDDEN_DIM
        
        self.multihead_attn = nn.MultiheadAttention(
            embed_dim=self.embed_dim,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )
        self.norm = nn.LayerNorm(self.embed_dim)
        
    def forward(self, query, key_value):
        # query: (batch, embed_dim) 
        # key_value: (batch, num_modalities, embed_dim)
        
        query = query.unsqueeze(1)  # (batch, 1, embed_dim)
        
        # Cross-attention
        attn_output, attn_weights = self.multihead_attn(
            query=query,
            key=key_value, 
            value=key_value
        )
        
        # Residual connection + layer norm
        output = self.norm(query + attn_output)
        
        return output.squeeze(1), attn_weights


class MultimodalCalorieModelWithAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # –ë–∞–∑–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.text_model = AutoModel.from_pretrained(config.TEXT_MODEL_NAME)
        self.image_model = timm.create_model(config.IMAGE_MODEL_NAME, pretrained=True, num_classes=0)
        
        # –ü—Ä–æ–µ–∫—Ü–∏–æ–Ω–Ω—ã–µ —Å–ª–æ–∏
        self.text_proj = nn.Linear(self.text_model.config.hidden_size, config.HIDDEN_DIM)
        self.image_proj = nn.Linear(self.image_model.num_features, config.HIDDEN_DIM)
        self.mass_proj = nn.Linear(1, config.HIDDEN_DIM // 4)
        
        # Cross-modal attention
        self.cross_attention = CrossModalAttention(config)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è –¥–ª—è –º–∞—Å—Å—ã (—á—Ç–æ–±—ã —Å–æ–≤–ø–∞–¥–∞–ª–∞ —Å HIDDEN_DIM)
        self.mass_proj_to_hidden = nn.Linear(config.HIDDEN_DIM // 4, config.HIDDEN_DIM)
        
        # –†–µ–≥—Ä–µ—Å—Å–æ—Ä
        fusion_dim = config.HIDDEN_DIM + config.HIDDEN_DIM // 4  # attn_output + original_mass
        self.regressor = nn.Sequential(
            nn.Linear(fusion_dim, config.HIDDEN_DIM),
            nn.LayerNorm(config.HIDDEN_DIM),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(config.HIDDEN_DIM, config.HIDDEN_DIM // 2),
            nn.ReLU(), 
            nn.Dropout(0.1),
            nn.Linear(config.HIDDEN_DIM // 2, 1)
        )

    def forward(self, input_ids, attention_mask, image, total_mass):
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        text_features = self.text_model(input_ids, attention_mask).last_hidden_state[:, 0, :]
        text_emb = self.text_proj(text_features)
        image_emb = self.image_proj(self.image_model(image))
        mass_emb = self.mass_proj(total_mass.unsqueeze(-1).float())
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–ª—é—á–∏-–∑–Ω–∞—á–µ–Ω–∏—è (—Ç–µ–∫—Å—Ç + –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)
        key_value = torch.stack([text_emb, image_emb], dim=1)  # (batch, 2, HIDDEN_DIM)
        
        # –ü—Ä–æ–µ–∫—Ç–∏—Ä—É–µ–º –º–∞—Å—Å—É –¥–ª—è attention (–∫–∞–∫ query)
        mass_query = self.mass_proj_to_hidden(mass_emb)  # (batch, HIDDEN_DIM)
        
        # Cross-modal attention
        attn_output, attn_weights = self.cross_attention(mass_query, key_value)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º output attention —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –º–∞—Å—Å—ã
        fused_emb = torch.cat([attn_output, mass_emb], dim=1)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞–ª–æ—Ä–∏–π
        calories = self.regressor(fused_emb)
        return calories.squeeze(-1), attn_weights


def validate(model, val_loader, device, mae_metric, rmse_metric=None, r2_metric=None):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫"""
    model.eval()
    total_loss = 0.0
    all_predictions = []
    all_labels = []
    val_attention_weights = []

    with torch.no_grad():
        for batch in val_loader:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            inputs = {
                'input_ids': batch['input_ids'].to(device),
                'attention_mask': batch['attention_mask'].to(device),
                'image': batch['images'].to(device),
                'total_mass': batch['total_mass'].to(device)
            }
            labels = batch['labels'].to(device)

            # Forward pass
            predictions, attn_weights = model(**inputs)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            mae_metric.update(predictions, labels)
            if rmse_metric:
                rmse_metric.update(predictions, labels)
            if r2_metric:
                r2_metric.update(predictions, labels)
            
            # Loss –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
            loss = nn.MSELoss()(predictions, labels)
            total_loss += loss.item()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            val_attention_weights.append(attn_weights.cpu().numpy())

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è R¬≤
    if len(all_predictions) < 2:
        print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è R¬≤ score (n={len(all_predictions)})")
    
    avg_loss = total_loss / len(val_loader)
    mae_score = mae_metric.compute().cpu().numpy()
    rmse_score = rmse_metric.compute().cpu().numpy() if rmse_metric else None
    r2_score = r2_metric.compute().cpu().numpy() if r2_metric and len(all_predictions) >= 2 else float('nan')
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º attention weights –∏–∑ –≤—Å–µ—Ö –±–∞—Ç—á–µ–π
    if val_attention_weights:
        val_attention_weights = np.concatenate(val_attention_weights, axis=0)
    
    return avg_loss, mae_score, rmse_score, r2_score, val_attention_weights


def load_checkpoint(model, optimizer, checkpoint_path, device):
    """–ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    if os.path.exists(checkpoint_path):
        print(f"üîÑ –ù–∞–π–¥–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: {checkpoint_path}. –ó–∞–≥—Ä—É–∂–∞–µ–º...")
        
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É
            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
        except:
            print("‚ö†Ô∏è  –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º weights_only=False")
            # –ï—Å–ª–∏ –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥
            # (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–æ–≤–µ—Ä—è–µ—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫—É —Ñ–∞–π–ª–∞!)
            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        if optimizer and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —ç–ø–æ—Ö–µ –∏ –º–µ—Ç—Ä–∏–∫–∞—Ö
        start_epoch = checkpoint.get('epoch', 0) + 1  # –Ω–∞—á–∏–Ω–∞–µ–º —Å–æ —Å–ª–µ–¥—É—é—â–µ–π —ç–ø–æ—Ö–∏
        best_mae = checkpoint.get('best_mae', float('inf'))
        
        print(f"‚úÖ –ß–µ–∫–ø–æ–∏–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω. –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —ç–ø–æ—Ö–∏ {start_epoch}")
        print(f"üìä –õ—É—á—à–∞—è MAE –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {best_mae:.1f} kcal")
        
        return start_epoch, best_mae
        
    else:
        print(f"üìù –ß–µ–∫–ø–æ–∏–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {checkpoint_path}. –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è.")
        return 0, float('inf')

def train(config, device, resume_training=True):
    seed_everything(config.SEED)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    visualizer = TrainingVisualizer()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    model = MultimodalCalorieModelWithAttention(config).to(device)
    tokenizer = AutoTokenizer.from_pretrained(config.TEXT_MODEL_NAME)

    # –†–∞–∑–º–æ—Ä–æ–∑–∫–∞ —Å–ª–æ–µ–≤ (–±–∞–∑–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è)
    set_requires_grad(model.text_model,
                      unfreeze_pattern=config.TEXT_MODEL_UNFREEZE, verbose=True)
    set_requires_grad(model.image_model,
                      unfreeze_pattern=config.IMAGE_MODEL_UNFREEZE, verbose=True)

    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    optimizer = AdamW([
        {'params': model.text_model.parameters(), 'lr': config.TEXT_LR},
        {'params': model.image_model.parameters(), 'lr': config.IMAGE_LR},
        {'params': list(model.text_proj.parameters()) + 
                   list(model.image_proj.parameters()) + 
                   list(model.mass_proj.parameters()) +
                   list(model.mass_proj_to_hidden.parameters()) +
                   list(model.cross_attention.parameters()) + 
                   list(model.regressor.parameters()), 
         'lr': config.CLASSIFIER_LR}
    ], weight_decay=0.01)

    criterion = nn.MSELoss()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    transforms = get_transforms(config)
    val_transforms = get_transforms(config, ds_type="val")
    train_dataset = MultimodalDataset(config, transforms)
    val_dataset = MultimodalDataset(config, val_transforms, ds_type="val")
    train_loader = DataLoader(train_dataset,
                              batch_size=config.BATCH_SIZE,
                              shuffle=True,
                              collate_fn=partial(collate_fn, tokenizer=tokenizer))
    val_loader = DataLoader(val_dataset,
                            batch_size=config.BATCH_SIZE,
                            shuffle=False,
                            collate_fn=partial(collate_fn, tokenizer=tokenizer))

    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
    mae_metric_train = torchmetrics.MeanAbsoluteError().to(device)
    mae_metric_val = torchmetrics.MeanAbsoluteError().to(device)
    rmse_metric_train = torchmetrics.MeanSquaredError(squared=False).to(device)
    rmse_metric_val = torchmetrics.MeanSquaredError(squared=False).to(device)
    r2_metric_train = torchmetrics.R2Score().to(device)
    r2_metric_val = torchmetrics.R2Score().to(device)
 
    # –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
    start_epoch = 0
    best_mae_val = float('inf')
    
    if resume_training and os.path.exists(config.SAVE_PATH):
        try:
            start_epoch, best_mae_val = load_checkpoint(model, optimizer, config.SAVE_PATH, device)
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {e}")
            print("üìù –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è")
            start_epoch = 0
            best_mae_val = float('inf')
    
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)

    print("Training started!")
    print("=" * 80)
    print(f"Target: Calorie Regression | Train samples: {len(train_dataset)} | Val samples: {len(val_dataset)}")
    print(f"Starting from epoch: {start_epoch}")
    print("=" * 80)
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –∏—Å–ø–æ–ª—å–∑—É–µ–º start_epoch –≤ —Ü–∏–∫–ª–µ
    for epoch in range(start_epoch, config.EPOCHS):
        model.train()
        total_loss = 0.0

        # –°–ë–†–ê–°–´–í–ê–ï–ú –ú–ï–¢–†–ò–ö–ò –ü–ï–†–ï–î –≠–ü–û–•–û–ô
        mae_metric_train.reset()
        rmse_metric_train.reset()
        r2_metric_train.reset()

        for batch_idx, batch in enumerate(train_loader):
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            inputs = {
                'input_ids': batch['input_ids'].to(device),
                'attention_mask': batch['attention_mask'].to(device),
                'image': batch['images'].to(device),
                'total_mass': batch['total_mass'].to(device)
            }
            labels = batch['labels'].to(device)

            # Forward
            optimizer.zero_grad()
            predictions, attn_weights = model(**inputs)
            loss = criterion(predictions, labels)

            # Backward
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            total_loss += loss.item()

            # –û–ë–ù–û–í–õ–Ø–ï–ú –º–µ—Ç—Ä–∏–∫–∏
            mae_metric_train.update(predictions, labels)
            rmse_metric_train.update(predictions, labels)
            r2_metric_train.update(predictions, labels)

        # –í–´–ß–ò–°–õ–Ø–ï–ú –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        avg_loss = total_loss / len(train_loader)
        train_mae = mae_metric_train.compute().cpu().numpy()
        train_rmse = rmse_metric_train.compute().cpu().numpy()
        train_r2 = r2_metric_train.compute().cpu().numpy()

        # –°–ë–†–ê–°–´–í–ê–ï–ú –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        mae_metric_val.reset()
        rmse_metric_val.reset()
        r2_metric_val.reset()

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        val_loss, val_mae, val_rmse, val_r2, val_attention_weights = validate(
            model, val_loader, device, mae_metric_val, rmse_metric_val, r2_metric_val
        )

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ scheduler
        scheduler.step(val_mae)

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        visualizer.update(epoch, avg_loss, train_mae, val_mae, train_rmse, val_rmse)
        
        # –ü–µ—á–∞—Ç—å –º–µ—Ç—Ä–∏–∫
        visualizer.print_current_metrics(epoch, avg_loss, train_mae, val_mae, train_rmse, val_rmse)
        
        # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤—ã–≤–æ–¥ R¬≤
        if not np.isnan(train_r2) and not np.isnan(val_r2):
            print(f"   R¬≤ Score - Train: {train_r2:.4f} | Val: {val_r2:.4f}")
        else:
            print(f"   R¬≤ Score - Train: N/A | Val: N/A (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö)")

        # –ê–Ω–∞–ª–∏–∑ attention weights (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if val_attention_weights is not None and len(val_attention_weights) > 0:
            avg_text_attention = val_attention_weights[:, 0, 0].mean()
            avg_image_attention = val_attention_weights[:, 0, 1].mean()
            print(f"   Attention - Text: {avg_text_attention:.3f}, Image: {avg_image_attention:.3f}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if val_mae < best_mae_val:
            best_mae_val = val_mae
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'epoch': epoch,
                'best_mae': best_mae_val
            }, config.SAVE_PATH)
            print(f"üöÄ New best model saved! Val MAE: {val_mae:.1f} kcal")

    print("=" * 80)
    print("Training completed!")
    print(f"Best validation MAE: {best_mae_val:.1f} kcal")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    visualizer.plot_metrics(save_path="training_metrics.png")
    visualizer.save_metrics_to_file()
    
    return visualizer, model


def predict_single(model, tokenizer, transforms, dish_data, device):
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ –±–ª—é–¥–∞"""
    model.eval()
    
    with torch.no_grad():
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        inputs = {
            'input_ids': tokenizer(
                dish_data['ingredients'], 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=512
            )['input_ids'].to(device),
            'attention_mask': tokenizer(
                dish_data['ingredients'], 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=512
            )['attention_mask'].to(device),
            'image': transforms(image=np.array(dish_data['image']))["image"].unsqueeze(0).to(device),
            'total_mass': torch.FloatTensor([dish_data['total_mass']]).to(device)
        }
        
        prediction, attn_weights = model(**inputs)
        return prediction.cpu().numpy()[0], attn_weights.cpu().numpy()


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    visualizer, trained_model = train(config, device, resume_training=True)